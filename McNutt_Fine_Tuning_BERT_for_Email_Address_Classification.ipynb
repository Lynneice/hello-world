{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzXSaMfxgTUXFGASsZCP1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lynneice/hello-world/blob/master/McNutt_Fine_Tuning_BERT_for_Email_Address_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline intended to enrich email data post-APRIMO sunset.\n",
        "\n",
        "\n",
        "\n",
        "*   audience_member_id flags extremely informative, underage/ooverage and other email eligiblity conditions.\n",
        "*   PEGA may not inherit exact same taxonomy, but if we can build out the next iteration of audience_member_id flags, PEGA may see better performance.\n",
        "*  With text-based classification in place, the goal is to enrich lead profile with additional signals based on information mined from email address alone.\n",
        "\n"
      ],
      "metadata": {
        "id": "MpjevU09v1AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "# !pip install transformers datasets scikit-learn pandas sklearn2pmml\n"
      ],
      "metadata": {
        "id": "3wS3fDPkvoa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn2pmml import sklearn2pmml  # Make sure this is imported\n",
        "from sklearn2pmml.pipeline import PMMLPipeline"
      ],
      "metadata": {
        "id": "D2E6Hd3c5oXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmzywfsIuKc9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy dataset\n",
        "# data = {\n",
        "#     'email': [\n",
        "#         'john.doe@gmail.com',\n",
        "#         'student@harvard.edu',\n",
        "#         'bot123@unknown.xyz',\n",
        "#         'tester@agency.gov',\n",
        "#         'vip@partner.com'\n",
        "#     ],\n",
        "#     'label': [0, 1, 2, 3, 4]  # Corresponding labels\n",
        "# }\n",
        "PATH = \"/content/email_classifier_model/training data/fake_emails_8k.csv\"\n",
        "df = pd.read_csv(PATH).sample(800, random_state=42)\n",
        "\n",
        "\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# # Split into training and test sets\n",
        "# train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "#     df['email'], df['label'], test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "\n",
        "# Create a new column with domain information\n",
        "df['domain'] = df['email'].str.split('@').str[1]\n",
        "\n",
        "# Split into training and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[['email', 'domain']], df['label'].values, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "lfl5cVqKuQUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Tokenize the data\n",
        "# train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "# test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize email and domain separately\n",
        "def tokenize_function(examples):\n",
        "    email_encodings = tokenizer(examples['email'], truncation=True, padding=True)\n",
        "    domain_encodings = tokenizer(examples['domain'], truncation=True, padding=True)\n",
        "\n",
        "    # Combine encodings (you might need to adjust how you combine them)\n",
        "    return {**email_encodings, **{k + '_domain': v for k, v in domain_encodings.items()}}\n",
        "\n",
        "# Apply tokenization\n",
        "train_encodings = tokenize_function(train_texts.to_dict(orient='list'))\n",
        "test_encodings = tokenize_function(test_texts.to_dict(orient='list'))"
      ],
      "metadata": {
        "id": "Z8wtYTsBuSvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['email'], df['label'].values, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "81Dy5VKJ3Z_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset class\n",
        "# class EmailDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, encodings, labels):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         item['labels'] = torch.tensor(self.labels[idx])\n",
        "#         return item\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "class EmailDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = EmailDataset(train_encodings, train_labels)\n",
        "test_dataset = EmailDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "kNsBw0PauU_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Initialization"
      ],
      "metadata": {
        "id": "PIlGxURluhN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model with a classification head\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yd8_71uuYmN",
        "outputId": "30ac793d-3bbd-4740-8408-5d18cadbc74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Setup"
      ],
      "metadata": {
        "id": "YEpYF6k3ujyE"
      }
    },
    {
      "source": [
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    # Replace 'evaluation_strategy' with 'eval_accumulation_steps' if using an older version\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    eval_accumulation_steps=1,  # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Initialize Tranier\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KMrNk321Yfi",
        "outputId": "56193a64-0215-4b6e-b170-cddafb2352b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-dc7ae5072f8e>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "qWMpEIbF3Sth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "traininng setup"
      ],
      "metadata": {
        "id": "5k1DsveAuo68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "trainer.train()\n",
        "\n",
        "#evaluate\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cVhH8rhRuqMi",
        "outputId": "ddbba58b-17d0-4329-a31e-d4739f0f6e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [480/480 19:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.484400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.998900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.969500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.576300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.349700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.151300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.050100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.026900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.012400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.008000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.005000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.003700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:12]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.0008769541746005416,\n",
              " 'eval_runtime': 13.0634,\n",
              " 'eval_samples_per_second': 12.248,\n",
              " 'eval_steps_per_second': 3.062,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model and tokenizer\n",
        "model.save_pretrained('./email_classifier_model')\n",
        "tokenizer.save_pretrained('./email_classifier_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SXDmifuw69",
        "outputId": "8e6aac93-4017-4e00-b17e-2b941da1fc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./email_classifier_model/tokenizer_config.json',\n",
              " './email_classifier_model/special_tokens_map.json',\n",
              " './email_classifier_model/vocab.txt',\n",
              " './email_classifier_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting to PMML - PEGA accepts pmml format but bert is huge lol </br>\n",
        "in lieu of wrappnig BERT models with SKL consider using formats like ONNX or TorchScript"
      ],
      "metadata": {
        "id": "tHzhYFnvu5bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikit-learn==1.5.2"
      ],
      "metadata": {
        "id": "5ZYxAgAo4eTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn2pmml import sklearn2pmml\n",
        "from sklearn2pmml.pipeline import PMMLPipeline\n",
        "\n",
        "# Create a simple pipeline\n",
        "vectorizer = CountVectorizer()\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "pipeline = PMMLPipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", classifier)\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the data\n",
        "pipeline.fit(df['email'], df['label'])\n",
        "\n",
        "# Export to PMML\n",
        "sklearn2pmml(pipeline, \"email_classifier.pmml\", with_repr=True)\n"
      ],
      "metadata": {
        "id": "CoNTJTXIu99M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the saved model\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('./email_classifier_model')\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained('./email_classifier_model')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntpTPfsdvOxW",
        "outputId": "3db26198-d328-4272-c940-210e8bfd222f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example email address\n",
        "email = \"johsnow@google.gurg\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(email, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# get the predicted class\n",
        "predicted_class = logits.argmax().item()\n",
        "print(f\"Predicted class: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wYqe-2x6o6F",
        "outputId": "2ebd1708-7028-4dce-aabe-3c6947ba4290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "map classes"
      ],
      "metadata": {
        "id": "sbTexSH1vcXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your label mapping\n",
        "label_mapping = {\n",
        "    0: \"VALID\",\n",
        "    1: \"EDU\",\n",
        "    2: \"BOT / BAD ACTOR / SUSPICIOUS\",\n",
        "    3: \"TEST / INTERNAL (.mil, .gov, etc.)\",\n",
        "    4: \"ON LIST / Known Member\"\n",
        "}\n",
        "\n",
        "# Get the label\n",
        "predicted_label = label_mapping[predicted_class]\n",
        "print(f\"Predicted label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srOvPoQivZVs",
        "outputId": "dd8a1708-bcaa-41af-af12-5e65f3d227d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: VALID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the pipeline\n",
        "classifier = pipeline(\"text-classification\", model='./email_classifier_model', tokenizer='./email_classifier_model')\n",
        "\n",
        "# Classify the email\n",
        "result = classifier(\"butt@los.gov\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbCamL7cvhWu",
        "outputId": "4f5855e0-1b7c-4b93-a83b-d02b2bc74226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_3', 'score': 0.9957094192504883}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Initialize the pipeline\n",
        "classifier = pipeline(\"text-classification\", model='./email_classifier_model', tokenizer='./email_classifier_model', return_all_scores=True)\n",
        "\n",
        "# Classify the email and get scores for all classes\n",
        "result = classifier(\"example@university.edu\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggG1BP6a6zz1",
        "outputId": "d997014e-f903-4efc-befb-0f1c1bfff513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'label': 'LABEL_0', 'score': 0.00020143704023212194}, {'label': 'LABEL_1', 'score': 0.9990304708480835}, {'label': 'LABEL_2', 'score': 0.00020884840341750532}, {'label': 'LABEL_3', 'score': 0.0002545579627621919}, {'label': 'LABEL_4', 'score': 0.00030479850829578936}]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier(\"marymary@ilstu.edu\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-yczs2y7GQh",
        "outputId": "939c9a16-82f4-476c-b16c-82b6ff58852d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_1', 'score': 0.9990909099578857}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier(\"john.snow@rapp.com\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7qwfvvo8RiL",
        "outputId": "b8272f85-7c82-49be-aa6b-964f8f5f3b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_0', 'score': 0.9992889165878296}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = classifier(\"mary.smith@grif.edu\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMwOC-os8kiB",
        "outputId": "e2aaef57-fde6-449d-b04f-f2eceef9a14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_1', 'score': 0.9990668892860413}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wz1uEjqp8oJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}